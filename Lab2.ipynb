{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: NLTK and spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be learning a bit about how to use the Python libraries <code>nltk</code> and ``spaCy`` to perform text normalization and to explore and analyze texts. \n",
    "\n",
    "The first few parts of this notebook will help you understand how to use Jupyter Notebooks, and there are many tutorials and quick-start guides on the web (<a href=\"https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/\">here</a>, <a href=\"https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\">here</a>, <a href=\"http://bi1.caltech.edu/code/t0b_jupyter_notebooks.html\">here</a>). Note that if you ever get weird behavior in a notebook. just go up to the Kernel menu and restart the kernel and clear the output, then run each code cell up to where you started having the problem.\n",
    "\n",
    "After you have completed all code and questions in this notebook, push and commit your version of this file along with the file you will create in part 7 to your repo. The deadline is Wednesday, September 14, at 11:59pm EDT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, where it says <code> In [ ]: </code>, type <code>print(\"Hello, World!\")</code>. Click in the cell below, and then hit the Run button from the menu of icons at the top of the page. Depending on your installation of jupyter, the run button might have the text Run or it might just be an icon that looks like a black triangle pointing to the right. The keyboard shortcut is <code>shift-return</code>, holding both keys down at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your Hello World code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underneath your command you should now see the output <code>Hello, World!</code>. \n",
    "\n",
    "Great! Now you have run your first command in this Jupyter Notebook. You can always go back and edit the stuff you've written in any code cell. Just remember to re-run it if you change anything. \n",
    "\n",
    "*Note: Many jupyter beginners forget that if you change the value of some variable in a block of code, that variable now has that new value everywhere -- even in earlier blocks of code. If you are having trouble, it often helps to go back and re-run the block of code where you originally set the value of that variable.* \n",
    "\n",
    "Now let's start using nltk. Start by typing <code>import nltk</code> in the command cell below to import the nltk library. Don't forget to hit the Run button above while the cursor is in the command cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your import nltk command here and hit Run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's likely that you don't have all the packages you need by default in nltk. Just in case, you should download the most popular ones. \n",
    "\n",
    "Run the code below **one time** to download the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: YOU ONLY NEED TO DO THIS ONCE!\n",
    "# When you run all the whole notebook at the end, you don't need \n",
    "# to execute this block. :)\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('popular')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If your whole computer crashes or things go really wrong, you'll need to download the packages as described in the README.md file in this repo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure your nltk is working, use it to calculate the minimum edit distance between two words. The function is <code>nltk.edit_distance</code> and the arguments are the two strings you want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your call to edit distance here and hit Run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're going to be downloading files a lot, you should also learn how to download files in Python. There are a few different libraries, but we'll be using <code>urllib</code>, so import that. We're also going to be using regular expressions, so we'll import that library, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your import urllib and re commands here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download a file. We're going to look at Great Expectations, by Charles Dickens. Click on the cell below and hit the Run button to issue the command to download the plain text version of the book from Project Gutenberg.\n",
    "\n",
    "**If you get a long error about a bad SSL certificate, just download the file directly and put it in your repo (and add, commit, and push it). We can work on getting downloads with urllib to work properly later.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: if you get an error, just download the file directly, rename it, \n",
    "# and add, commit, and push it to your repo\n",
    "\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/1400/1400-0.txt\", \"greatexpectations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a text to work with. In the same directory where you saved this Notebook, you should see the file you just downloaded saved as <code>greatexpectations.txt</code>. Using whatever text editor you like (on a Mac, it will open by default with TextEdit.app), have a look at the file, and familiarize yourself with the format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that plain text Gutenberg Project books are formatted to have 80 or fewer characters per line. This is fine for reading on an old-timey computer screen, but when we're processing text, we don't want a lot of manually inserted hard line breaks in the middle of our text. We're going to read in the text and replace line breaks with spaces. Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"greatexpectations.txt\", \"r\", encoding=\"utf-8\")\n",
    "alltext = f.read().rstrip()\n",
    "alltext = re.sub(\"\\n\", \" \", alltext)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>alltext</code> is a single string containing the entire text of the book. You can see that this is true by printing out the whole thing, but that will take up lots of space. Instead just try printing a random slice, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alltext[0:25])\n",
    "print(alltext[-99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from when you examined the file in a text editor that there there was a bunch of text at the beginning and end of the file that was not actually a part of the text of the book. Above I showed how to use <code>re.sub</code> to remove all the line breaks. In the cell below, use <code>re.sub</code> to delete everything up to and including ``Chapter I.   `` **followed by three spaces**. Then use <code>re.sub</code> to delete everything starting from the white space that appears before ``*** END OF THE PROJECT GUTENBERG EBOOK GREAT EXPECTATIONS ***`` all the way to the end of the file. \n",
    "\n",
    "Hint: Be very careful about spaces, case, punctuation, etc. Some regular expressions you will find very useful: <code>+ ^ $ \\s .\\*</code> and <code>.\\*?</code> and the backslash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here and run it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did your regular expressions right, repeating the slice printing commands above will yield the following output:\n",
    "\n",
    "<code>My fatherâ€™s family name b</code><br> \n",
    "<code>the broad expanse of tranquil light they showed to me, I saw no shadow of another parting from her.</code>\n",
    "\n",
    "<b>If you didn't get this output, *go back and reload the file* by putting the cursor in the command cell where you originally read in the file and clicking Run.</b> Then try your regular expression again. Do not continue until you get the right output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can turn a \"sentence\" into a string of \"words\" by splitting on white space using the <code>split</code> function. As we've discussed in class, however, splitting on white space is not a great way to tokenize (i.e., to separate out each actual word) because you leave punctuation attached to words. This prevents you from recognizing that, for instance, \"dogs\" is the same word whether it's before a space or a comma. In addition, you won't be able to learn anything about the distribution of different punctuation marks since they will always be attached to something else.\n",
    "\n",
    "Fortunately, nltk has a word tokenizer function that, when given a string, will return a list of tokens. Here's the syntax for calling it:\n",
    "\n",
    "<code>listoftokens = nltk.word_tokenize(inputstring)</code>\n",
    "\n",
    "Call this function on <code>alltext</code> to produce a list of tokens called <code>alltokens</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call nltk.word_tokenize here and Run\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Q1: How many tokens are there in this text? How many types are there in this text? What is the type:token ratio? Write three python commands in the line below that will calculate these three numbers. Then print out all three numbers.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line of code for token count\n",
    "\n",
    "\n",
    "# line of code for type count\n",
    "\n",
    "\n",
    "# line of code for type:token ratio\n",
    "\n",
    "\n",
    "# line of code to print out all three\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Q2: What text normalization might you want to do before counting the number of types and tokens? (Hint: there are some words you might be counting as separate types because of the way they are spelled.) How might this normalization make your type and token counts more accurate? How might it make these counts less accurate?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click here to enter your answers to Q2 \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Frequency distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers to Q1 demonstrate that there must be some words that were used more than once. Suppose you want to know what are the most frequent words. You can do this using the <code>FreqDist()</code> class in nltk. Run the code below to create a frequency distribution for your list of tokens and to print out the 10 most frequent words and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(alltokens)\n",
    "fdist.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too surprising that the words you see in this list are the most common words. These little words that don't add a lot of content to language but appear frequently and usually serve a specific function are called <i><b>function words</i></b> or <i><b>closed class words</i></b>. These words are important, but the don't tell us much by themselves about the story.\n",
    "\n",
    "What should we do if we want to know the most frequent words that are <i><b>content words</b></i> or <i><b>open class words</b></i> like nouns, verbs, adjectives, and adverbs -- the kinds of words that can tell us more about the story itself?\n",
    "\n",
    "We filter out the function words using a <i><b>stop list</b></i>, which is a list of words that we can skip when we're interested in the real content of a text. nltk provides a stop list that you can use and add to. Let's get it and print it out to see what's there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "print(stoplist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: What common and important class of tokens is missing from this list that we also might like to ignore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click here to answer Q3 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add at least three of these missing tokens to the stop list using the usual Python syntax for appending to or extended a list, and check to make sure it worked. Then make a new version of <code>alltokens</code> from which all stop words in your stoplist have been removed. Finally, create a new <code>FreqDist</code> from this stopword-free list of tokens, and print out the top 10 tokens.\n",
    "\n",
    "Keep adding stop words (or stop tokens!) to the stoplist until you start seeing mostly real content words in the top 10.\n",
    "\n",
    "(Note: There are smart quotes in the text because it's UTF-8 not ascii. You can add these to the stoplist by just copying and pasting them into your list of things you're adding to the stoplist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# enter your code for appending at least three tokens to the stop list here\n",
    "\n",
    "\n",
    "\n",
    "# print out the stoplist to make sure your new tokens were added correctly\n",
    "\n",
    "\n",
    "\n",
    "# make a new version of alltokens called allcontenttokens that doesn't contain items from the stop list\n",
    "\n",
    "\n",
    "\n",
    "# create a new FreqDist from this new version of allcontenttokens\n",
    "\n",
    "\n",
    "\n",
    "# print out the top 10 most frequent tokens in this new FreqDist\n",
    "\n",
    "\n",
    "\n",
    "# Remember to repeat the above steps until the 10 most frequent words are content words\n",
    "# rather than function words or punctuation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: How many tokens did you have to add to the stoplist? What do you think of nltk's stoplist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click here to answer Q4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we learned about language modeling with n-grams. nltk makes counting n-grams really easy with <code>nltk.util.ngrams</code>. \n",
    "\n",
    "**Note: I am using ``alltokens`` here and not ``allcontenttokens``. Why? Because language models are used when we are interested in word *sequences and how words fit together with each other*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "mybigrams = ngrams(alltokens,2)\n",
    "mytrigrams = ngrams(alltokens,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a <code>FreqDist</code> for the bigrams and trigrams in the text, and print out the 10 most frequent for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code for creating FreqDist for bigrams and trigrams\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with bigrams in nltk, you can also build a <i>conditional</i> frequency distribution, which, for a given word, keeps track of the frequencies of any following words. Let's look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicfreq = nltk.ConditionalFreqDist(ngrams(alltokens,2))\n",
    "\n",
    "print(bicfreq[\"Mr.\"].most_common(10))  # prints out common words after Mr.\n",
    "\n",
    "print(bicfreq[\"who\"].most_common(10))  # prints out common words after who\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the <code>FreqDist</code> class is useful, nltk's language modeling functionality is very buggy, so we probably won't be using it for this class. We'll be using other tools, one of which you'll learn about in future labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a common normalization task we haven't performed yet: stemming or lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Looking at the top 50 or 100 most frequent unigrams, how can you tell the tokens are not stemmed or lemmatized? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click here to answer Q5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command cell below shows how to use nltk's only true lemmatizer, the WordNet Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# an example\n",
    "print(lemmatizer.lemmatize(\"dogs\"))\n",
    "print(lemmatizer.lemmatize(\"speaks\", \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this lemmatizer to lemmatize every token in the <code>allcontenttokens</code> list you created above. Then make a new frequency distribution and examine the 50 or 100 most frequent words.\n",
    "\n",
    "**Note: I want you to use ``allcontenttokens`` here. Why? Because we are thinking about words by themselves here rather than word sequences so we can disregard function words. In addition, you can lemmatize only verbs, nounsm adjectives, and adverbs (in English, at least).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list of tokens, all_lemmas by lemmatizing allcontenttokens\n",
    "\n",
    "\n",
    "\n",
    "# Build a new FreqDist on all_lemmas and print out the 50 or 100 \n",
    "# most frequent lemmatized tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It probably doesn't look much better. This is because the WordNet lemmatizer in nltk assumes by default that every word is a noun. Unless you tell the lemmatizer that something is a verb, it won't try to look it up as a verb. This is why \"said\" doesn't get lemmatized, and also why \"was\" gets lemmatized to \"wa\". In a future lab or problem set, we'll be exploring automatic part of speech tagging, which allows us to label every word as a noun, verb, adjective, preposition, etc. We'll also see shortly that spaCy does a much better job with this.\n",
    "\n",
    "Note that there are several different stemmers implemented in the nltk.stem package. You can explore these in the nltk.stem package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of this lab, you'll need to take this text and save it out to a file with one tokenized sentence per line. Let's start by going back to the string holding our original text, <code>alltext</code>. We can turn this into a list of strings, each of which is a sentence, using the <code>sent_tokenize()</code> function, which takes a string as an argument and returns a list of sentences.\n",
    "\n",
    "Below, take <code>alltext</code>, break it up into sentences with <code>sent_tokenize()</code>. Then loop through the sentences in that list, and use <code>word_tokenize</code> to tokenize each sentence. Print each tokenized sentence out to a file so that you have one sentence per line. \n",
    "\n",
    "**Note: Do not just call <code>print()</code>! This will print out an ugly list of lists. Cycle through the lists to print out strings.** \n",
    "\n",
    "For example, this text:\n",
    "\n",
    "<code>Open the pod bay doors, Hal! I'm sorry, Dave. I can't do that.</code>\n",
    "\n",
    "would get printed to a file as this:\n",
    "\n",
    "<code>Open the pod bay doors , Hal !</code><br>\n",
    "<code>I'm sorry , Dave .</code><br>\n",
    "<code>I can't do that .</code><br>\n",
    "\n",
    "Please observe that there are *no quotes, no square brackets, and no commas, as you would get if you just called ``print()`` on a Python list!* TAs will be instructed to give you a 0 for this section if you print out a raw list. \n",
    "\n",
    "Name the file you print out to <code>great.txt</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use sent_tokenize() to break alltext into a list of sentences, allsent\n",
    "\n",
    "\n",
    "# Open a file to write to called great.txt.\n",
    "\n",
    "\n",
    "# Loop through the sentences\n",
    "\n",
    "    # call word_tokenize() on each sentence\n",
    "\n",
    "    # First write out <s> to the file to indicate the beginning of a sentence, then a space.\n",
    "    \n",
    "    # Then write out to the file each token one-by-one, each followed by a space. \n",
    "   \n",
    "    # Then write out </s> to indicate the end of the sentence.\n",
    "    \n",
    "    \n",
    "# Close the file great.txt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second line of your file should look like this:\n",
    "\n",
    "```\n",
    "<s> So , I called myself Pip , and came to be called Pip . </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using spaCy to do a lot of this work for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial that you understand how to do each of these steps yourself since which steps you do depends very much on what task you are working on. Things like capitalization, punctuation, function words, and sentence boundaries might be important for what you want to do, or they might not matter at all.\n",
    "\n",
    "However, there is a different python library, spaCy, that will do a lot of this for you (and more!) automagically (and more slowly). Experiment with the code below to see the different things spaCy can do. To explore more, you can consult [the official spaCy documentation](https://spacy.io/api) or helpful websites like [this](https://realpython.com/natural-language-processing-spacy-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cool jupyter feature: Putting an exclamation point at the beginning of a line in \n",
    "# in a jupyter notebook lets you run a lot of commands that you would normally\n",
    "# run at a linux command line.\n",
    "!python3.9 -m spacy download en_core_web_sm  # Comment this out after you run it for the first time.\n",
    "\n",
    "# This line loads a big model/pipeline that works specifically for English.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Remember: spaCy is fancy, so it can be slow. Let's look at just the \n",
    "# first 10000 characters of Great Expectations.\n",
    "\n",
    "doc = nlp(alltext[0:10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline you loaded in line 9 in the above code block, which I called ``nlp``, takes as input a text. It then returns a data structure that contains a very detailed processing and analysis of that text, including sentence boundary detection, tokenization, lemmatizing, part of speech  tagging, and all kinds of other helpful things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to get access to sentences.\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to get tokens, along with information \n",
    "# about each token such as its lemma and part of speech.\n",
    "for token in doc:\n",
    "    print(token, token.lemma_, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spaCy has stoplists, too, and they are much more\n",
    "# complete and expansive (perhaps too expansive)\n",
    "# than the nltk list\n",
    "\n",
    "english_stops = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some practice using spaCy, I'd like you to try it out some of the above commands but with a new text in a language that is *not* English!\n",
    "\n",
    "**Step 1**: There are many trained pipelines for different languages here: https://spacy.io/models. Go to that address, and pick a language. In the code box below where you pick the language, you'll see a line that shows you how to load a pipeline for the language you have selected (e.g., ``nlp = spacy.load(\"es_core_news_sm\")`` for Spanish).\n",
    "\n",
    "**Step 2**: Go on the web and find a chunk of text for the language you chose. You can pick text from Gutenberg or from Google news for that langauge or from any website where you can get a good continuous chunk of 100-200 words. \n",
    "\n",
    "**Step 3**: Process that chunk of text with the language pipeline you chose in Step 1. (You can just copy and paste the chunk into your code block.)\n",
    "\n",
    "**Step 4**: Print out the following:\n",
    "* The number of tokens in the text.\n",
    "* The number of sentences in the text.\n",
    "* All the verbs in the text.\n",
    "* All the stopwords in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for Part 8 here. Do not forget to include comments!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verifying and submitting your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've answered every <b>Q</b> question.\n",
    "\n",
    "Make sure you've written code wherever required. \n",
    "\n",
    "Go up to the Kernel menu and select Restart and Run All. **(Don't forget that you can comment out or skip the nltk download block and the urllib block.)** This will run all of the code you've written. Make sure there are no errors.\n",
    "\n",
    "Add, commit, and push this file and the <code>great.txt</code> file to your repo. When you are totally done, make the comment say \"FINAL SUBMISSION - PLEASE GRADE\".\n",
    "\n",
    "This lab is due September 14, 2022, at 11:59pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
